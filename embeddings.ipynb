{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings in the sentence balancing task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How much does positive,negative adjective pairs correlated compared to random adjective pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c385e627b0e84ebeac36d603135ea41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e94004bfa649eea60fd3bbbb4643f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r1_adj=[\"rich\",\"smart\",\"ugly\",\"fast\",\"long\",\"cheap\",\"clear\"]\n",
    "r2_adj=[\"bright\",\"cold\",\"sweet\",\"cold\",\"bad\",\"weak\",\"good\"]\n",
    "\n",
    "pos_adj=[\"bright\",\"hot\",\"sweet\",\"warm\",\"bad\",\"strong\",\"good\"]\n",
    "neg_adj=[\"dark\",\"cold\",\"bitter\",\"cold\",\"good\",\"weak\",\"bad\"]\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import numpy as np\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "import torch\n",
    "emb_pos=torch.empty((768,7))\n",
    "emb_neg=torch.empty((768,7))\n",
    "r1_emb=torch.empty((768,7))\n",
    "r2_emb=torch.empty((768,7))\n",
    "pos_neg_pair=list()\n",
    "r1_r2_pair=list()\n",
    "for i in range(len(pos_adj)):\n",
    "\n",
    "    pos_adj_id = tokenizer.encode(\" \"+pos_adj[i], return_tensors='pt')\n",
    "    neg_adj_id = tokenizer.encode(\" \"+neg_adj[i], return_tensors='pt')\n",
    "    r1_adj_id = tokenizer.encode(\" \"+r1_adj[i], return_tensors='pt')\n",
    "    r2_adj_id = tokenizer.encode(\" \"+r2_adj[i], return_tensors='pt')\n",
    "    pos_adj_emb = model.wte(pos_adj_id).detach().reshape(-1).numpy()\n",
    "    neg_adj_emb = model.wte(neg_adj_id).detach().reshape(-1).numpy()\n",
    "    emb_pos[:,i]=model.wte(pos_adj_id).detach()\n",
    "    emb_neg[:,i]=model.wte(neg_adj_id).detach()\n",
    "\n",
    "    r1_adj_emb = model.wte(r1_adj_id).detach().reshape(-1)\n",
    "    r2_adj_emb = model.wte(r2_adj_id).detach().reshape(-1)\n",
    "    r1_emb[:,i]=r1_adj_emb\n",
    "    r2_emb[:,i]=r2_adj_emb\n",
    "    pos_neg_pair.append(np.corrcoef(neg_adj_emb,pos_adj_emb)[0][1])\n",
    "    r1_r2_pair.append(np.corrcoef(r1_adj_emb,r2_adj_emb)[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pairwise cosine similarity: 0.0027156149655636415\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = model.wte.weight\n",
    "batch_size = 500\n",
    "\n",
    "# Variable to accumulate cosine similarities\n",
    "total_cosine_similarity = 0.0\n",
    "\n",
    "# Calculate mean pairwise cosine similarity in batches to avoid memory overflow\n",
    "for i in range(0, word_embeddings.size(0), batch_size):\n",
    "    batch = word_embeddings[i:i+batch_size, :]\n",
    "    norm = batch.norm(p=2, dim=1, keepdim=True)\n",
    "    batch_normalized = batch.div(norm)\n",
    "    similarity_matrix = torch.mm(batch_normalized, batch_normalized.transpose(0,1))\n",
    "    \n",
    "    # Exclude self-similarities (diagonal elements) and duplicates (upper triangular elements)\n",
    "    similarity_sum = torch.tril(similarity_matrix, diagonal=-1).sum()\n",
    "    \n",
    "    # Update total cosine similarity\n",
    "    total_cosine_similarity += similarity_sum.item()\n",
    "\n",
    "# Calculate mean cosine similarity\n",
    "num_pairs = (word_embeddings.size(0)*(word_embeddings.size(0)-1))/2\n",
    "mean_cosine_similarity = total_cosine_similarity / num_pairs\n",
    "\n",
    "print(f\"Mean pairwise cosine similarity: {mean_cosine_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(emb_neg).to_csv(\"neg_emb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation of random adjectives: 0.3393381970372445\n",
      "Mean correlation of positive and negative adjectives: 0.5797392398466\n"
     ]
    }
   ],
   "source": [
    "mean_cor_pos_neg=np.array(pos_neg_pair).mean()\n",
    "mean_cor_random=np.array(r1_r2_pair).mean()\n",
    "\n",
    "print(f\"Mean correlation of random adjectives: {mean_cor_random}\")\n",
    "print(f\"Mean correlation of positive and negative adjectives: {mean_cor_pos_neg}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well does regression change the polarity of the adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "coefficients = []\n",
    "X=emb_pos\n",
    "y=emb_neg\n",
    "# Fit a linear regression model without intercept for each feature\n",
    "for feature_index in range(X.shape[1]):\n",
    "    # Select the current feature\n",
    "    X_feature = X[:, feature_index].reshape(-1, 1)\n",
    "    y_t=y[:,feature_index]\n",
    "    # Create a linear regression object without intercept\n",
    "    regressor = LinearRegression(fit_intercept=False)\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    regressor.fit(X_feature, y)\n",
    "\n",
    "    # Get the coefficient for the current feature\n",
    "    coefficient = regressor.coef_[0]\n",
    "\n",
    "    # Add the coefficient to the list\n",
    "    coefficients.append(coefficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_new=emb_pos[3,:]\n",
    "predictions = np.zeros(X_new.shape[0])\n",
    "\n",
    "# Make predictions using the obtained coefficients\n",
    "for feature_index in range(768):\n",
    "    # Select the current feature\n",
    "    X_feature = X_new[feature_index]\n",
    "\n",
    "    # Multiply the feature by its corresponding coefficient and add it to the predictions\n",
    "    predictions[feature_index]=X_feature * coefficients[feature_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: bring, Similarity: 0.13129960579542277\n",
      "Token:  defunct, Similarity: 0.12518269726551684\n",
      "Token:  presumed, Similarity: 0.11389661342921062\n",
      "Token: ACH, Similarity: 0.1128229888463981\n",
      "Token: aum, Similarity: 0.11199317371042172\n",
      "Token:  Eternity, Similarity: 0.11149686015522803\n",
      "Token: imaru, Similarity: 0.11114468272398045\n",
      "Token: ixt, Similarity: 0.11080637292754249\n",
      "Token: had, Similarity: 0.11068742658949278\n",
      "Token: rites, Similarity: 0.11049747390073628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "word_embeddings = model.wte.weight\n",
    "\n",
    "# Convert to numpy for easier calculations\n",
    "word_embeddings_np = word_embeddings.detach().numpy()\n",
    "\n",
    "# This should be your predicted embedding\n",
    "predicted_embedding = predictions\n",
    "\n",
    "# Reshape it to 2D because cosine_similarity expects 2D arrays\n",
    "predicted_embedding = predicted_embedding.reshape(1, -1)\n",
    "\n",
    "# Compute the cosine similarities\n",
    "similarities = cosine_similarity(predicted_embedding, word_embeddings_np)\n",
    "\n",
    "# Get the indices of the top 10 closest tokens\n",
    "top_10_indices = similarities[0].argsort()[-10:][::-1]\n",
    "\n",
    "# Print the closest tokens\n",
    "for index in top_10_indices:\n",
    "    token = tokenizer.decode([index])\n",
    "    print(f\"Token: {token}, Similarity: {similarities[0][index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
